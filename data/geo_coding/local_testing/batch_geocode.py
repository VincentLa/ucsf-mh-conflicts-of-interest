""" Geocode 1000 batches of addresses at a time. """

import os
import pandas as pd
import requests
import sqlalchemy as sqla
import sys
import tempfile
import subprocess
import threading
from queue import Queue
import time
import queue
import logging
import concurrent.futures


pd.options.display.max_columns = 999

import censusgeocode as cg


"""
Directions: 
Input_queue := pre-called address
output_queue = tuple(address, geocode)


worker:
    1. call address from the input_queue
    2. api call and get geocodes using the address in the input_queue
    3. input (address, geocode) into the output_queue
master:
    1. put addresses to the input_queue
    2. extract (address, geocode) from the output_queue and add it to the row of the pandas dataframe


Tasks needed to be programmed:
    1. Worker & functions
    2. Master & functions
    3. Master inputs the addresses to the input_queue
    4. Workers deployed in threads
    5. Master aggregates the information from various workers
     

parameters: n_thread, batchsize, timeout_specification

"""


def wrapper_api_call(df_input_addresses_queued):
    """
    API Wrapper (in this case censusgeocode pkg) call to return geo_codes.
    For censusgeocode pkg, input is a list of dict (input batch has to be less than 1000 addresses)
        and the output will be a list of [original address df, OrderedDict with geocodes].
    Dictionary has to have columns as street, city, state, and zip. (ID autogenerated in censusgeocode pkg).
    """
    logging.info("Worker is now working.")
    print("Worker is now working.")
    geo_codes_ordered_dict = cg.addressbatch(df_input_addresses_queued.to_dict('records'))
    logging.info("Worker's work is complete.")
    print("Worker's work is complete.")
    return geo_codes_ordered_dict


def work(current_queue):
    while not current_queue.empty():
        logging.info("Fetching a job from the queue...")
        print("Fetching a job from the queue...")
        df_input_addresses_queued = current_queue.get()
        logging.info("Ordering the worker to start the work...")
        print("Ordering the worker to start the work...")
        return wrapper_api_call(df_input_addresses_queued)


class Master:
    """ Instantiating a Master class"""

    def __init__(self, df_input_addresses, n_threads, timeout_sec):
        """
        Master_input_df is the master dataframe with all the addresses that need to be geo-coded.
        """
        self.df_input_addresses = df_input_addresses
        self.df_input_addresses_queued = pd.DataFrame(columns=list(df_input_addresses))
        self.n_threads = n_threads
        self.timeout_sec = timeout_sec

    def load_up_queue(self, current_queue, df_index, df_input_addresses_queued):
        logging.info("Loading up the queue with an address data frame")
        print("Loading up the queue with an address data frame")
        current_queue.put((df_index, df_input_addresses_queued))

    def geo_codes_into_df(self, geo_codes_ordered_dict):
        logging.info("Changing result OrderedDict object to a pandas DataFrame")
        print("Changing result OrderedDict object to a pandas DataFrame")
        geo_codes_result_df = (pd.DataFrame.from_dict(geo_codes_ordered_dict))
        return geo_codes_result_df


def run_geocode_api(n_threads, input_df, timeout_sec, queue_size=5, queue_df_size=200):
    """
    Main function to run the geocoding using number of threads pre-defined.
    n_threads: number of threads that the task will run on
    input_df: dataframe that contains ALL the addresses (columns: street, city, state, zip)
    timeout_sec: seconds it will take to timeout the API call
    queue_size = the total queue size (not the dataframe length, but the number of dataframe that will be stored in
        the queue.Queue object.
    """
    current_queue = queue.Queue(maxsize=queue_size)
    master = Master(input_df, n_threads, timeout_sec)
    logging.info("Changing result OrderedDict object to a pandas DataFrame")
    print("Initializing the Output table by calling the wrapper on 2 rows.")
    geo_codes_ordered_dict = wrapper_api_call(df_input_addresses_queued=input_df.iloc[0:2, :])
    col_df = master.geo_codes_into_df(geo_codes_ordered_dict)
    master_output_df = pd.concat([input_df, pd.DataFrame(columns=list(col_df), index=range(0, len(input_df)))])
    print("Initialized the master Output table.")
    print("Starting the loop.")
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as executor:
        for x in range(0, input_df.shape[0], queue_df_size):
            if x + queue_df_size <= input_df.shape[0]:
                df_address = input_df.iloc[x: x + queue_df_size, :]
                current_queue.put(df_address)
                df_input_addresses_queued = current_queue.get()
                geo_codes_ordered_dict_local = wrapper_api_call(df_input_addresses_queued)
                # geo_codes_ordered_dict_local = executor.submit(work, current_queue, event).result(timeout_sec)
            else:
                df_address = input_df.iloc[x: input_df.shape[0], :]
                current_queue.put(df_address)
                df_input_addresses_queued = current_queue.get()
                geo_codes_ordered_dict_local = wrapper_api_call(df_input_addresses_queued)
                # geo_codes_ordered_dict_local = executor.submit(work, current_queue, event).result(timeout_sec)
            geo_codes_result_df = master.geo_codes_into_df(geo_codes_ordered_dict_local)

            master_output_df = master_output_df.append(geo_codes_result_df,
                                                       sort=True,
                                                       ignore_index=True)
        return master_output_df




#################################################################################


import time
import threading
import random

from queue import Queue
from queue import Empty


def get_dummy_data(size=100):
    return list(range(0, size))


def calculate(number):
    time.sleep(random.randint(0, 100) / 400)  # simulate
    return number * number


def main(n_thread=1):
    print("Get dummy data, a python list.")
    data = get_dummy_data()

    # We run differently depending on the number of threads.
    if n_thread < 1:
        raise ValueError("There must be a non-zero number of threads.")
    elif n_thread == 1:
        print("Running single thread.")
        process_single(data)
    else:
        print("Running multiple threads.")
        process(data, n_thread)


def process_single(data):
    """
    Single thread processing.
    """
    start = time.time()
    result = []
    for index, number in enumerate(data):
        print("Processing {} of {}...".format(index + 1, len(data)))
        result.append(calculate(number))
    print("Done - " + str(time.time() - start))
    print(result)


def work(thread_id, input_queue, output_queue, event):
    # Until we have a "done" signal AND the input queue is empty, we run.
    while not event.is_set() or not input_queue.empty():
        try:  # Try to get the input
            index, number = input_queue.get(timeout=0.5)
        except Empty:  # If we get an empty exception, we loop
            continue
        else:  # success, we calculate and put
            print("[W-" + str(thread_id) + "] Calculating: " + str(index) + ", " + str(number))
            result = calculate(number)
            output_queue.put((index, result))  # add to the output queue.


def process(data, n_thread):
    start = time.time()
    input_queue = Queue()  # Inpue queue, e.g. for addresses
    output_queue = Queue()  # Output queue, e.g. for geopoints
    event = threading.Event()  # flag set IFF all inputs have ben enqueued.

    left = len(data)  # How many data points left?
    data_index = 0  # index of the processing input.

    result = [None for _ in data]  # None as the placeholder for results.

    # Step 1 - spawn worker threads to wait for the queue and output results.
    for i in range(n_thread):  # spawn n threads.
        t = threading.Thread(target=work, args=(i, input_queue, output_queue, event))
        t.start()  # start off!

    # Step 2 - While not done, input the data (if available), and process the results.
    while left > 0:  # We do "something" while we have work to do.
        if not event.is_set():  # If we are NOT done enqueuing all the input data.
            if data_index < len(data):  # If we have more to enqueue
                number = data[data_index]
                print("[Main] Putting: " + str(data_index) + ", " + str(number))
                input_queue.put((data_index, number))
                data_index += 1
            else:  # We are done enqueueing the input.
                print("[Main] Done enqueueing.")
                event.set()  # workers can quit the while loop as the queue empties.

        while not output_queue.empty():  # While we have output data to process,
            index, number = output_queue.get()
            result[index] = number  # Put the numbers in the correct spot
            left -= 1

    print("Done - " + str(time.time() - start))
    print(result)


if __name__ == "__main__":
    main(n_thread=100)
